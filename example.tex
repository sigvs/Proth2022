%===============================================================================
% $Id: ifacconf.tex 19 2011-10-27 09:32:13Z jpuente $  
% Template for IFAC meeting papers
% Copyright (c) 2007-2008 International Federation of Automatic Control
%===============================================================================
\documentclass{ifacconf}
%\usepackage[cp1251]{inputenc}
%\usepackage[russian]{babel}
\usepackage{graphicx}      % include this line if your document contains figures
\usepackage{tikz}
\usepackage{natbib}        % required for bibliography
%\usepackage[ifacconf}

%===============================================================================
\begin{document}
\begin{frontmatter}

\title{On Local Optima Distribution in Buffer Allocation Problem for Production Line with Unreliable Machines\thanksref{footnoteinfo}} 
% Title, preferably not more than 10 words.

\thanks[footnoteinfo]{The research is supported by Russian Science Foundation  grant~21-41-09017.}

\author[First]{Alexandre Dolgui} 
\author[Second]{Anton Eremeev} 
\author[Third]{Viatcheslav Sigaev}

\address[First]{IMT Atlantique, Nantes, France}
\address[Second]{Sobolev Institute of Mathematics SB RAS, Novosibirsk, Russia (e-mail: eremeev@ofim.oscsbras.ru).}
\address[Third]{Avtomatika-Servis LLC, Omsk, Russia (e-mail: sigvs@yandex.ru).}


\begin{abstract}                % Abstract of not more than 250 words.
In this paper, we consider a manufacturing flow-line
organized as a series-parallel system of machines separated by
finite buffers. The failure and repair times of machines are
supposed to be exponentially distributed. The production rate of
each machine is deterministic, and different machines may have
different production rates. The buffer allocation problem consists
in determining the buffer capacities with respect to a given
optimality criterion, which depends on the average production rate
of the line, the buffer acquisition and installation cost and the
inventory cost. The tentative solutions are evaluated with an
approximate method based on the Markov models aggregation
approach. The computational experiments show better quality of
solutions obtained by a genetic algorithm compared with the local
descent and tabu-search algorithms. It is indicated that in many
test problems several clusters of local optima can be found.
\end{abstract}

\begin{keyword}
Production line, Unreliable machines, Buffer allocation, Series-parallel network, Genetic algorithms, Local optima.
\end{keyword}

\end{frontmatter}
%===============================================================================

\section{Introduction}

Buffer  capacity  allocation  problems  arise  in  a  wide  range  of  manufacturing  systems, 
such  as  transfer  lines,  flexible  manufacturing  or  robotic  assembly  systems  which  are 
flow lines. Buffers separate any two consecutive machines. The parts are accumulated in
a buffer when the machines downstream are less productive than machines upstream. 
Assume that machines can breakdown. When a breakdown occurs, the corresponding 
machine is not used in production for a random repair time which is independent on the 
number of failed machines. It is assumed that there is a sufficient number of raw parts at 
the input of the system and these parts are always available. The completed parts depart 
from the system immediately. The performance of the flow-line is measured in terms of 
the average production rate, i.e., the steady state average number of parts produced per 
unit of time.

In the literature, there are two types of publications. The first concerns only evaluation 
of the line performance for a given size of buffers. In the second, the buffer sizes are 
optimized. For example, (Dallery and Gershwin, 1992), (Gershwin, 1993), (Heavey et 
al., 1993), (Meerkov and Li, 2008), and (Tan and Gershwin, 2009) proposed models to 
evaluate the performance of lines with unreliable machines and fixed sizes of buffers. 
Markov models and aggregation or decomposition techniques are often used to calculate 
steady state throughput or other performance indicators for these lines provided that the 
buffer  capacities  are  given.  Based  on  these  models  for  performance  analysis,  in,  e.g., 
(Smith  and  Daskalaki,  1988),  (So,  1997),  (Gershwin  and  Schor,  2000),  and  (Shi  and 
Gershwin,  2009),  the  optimization  for  buffer  capacity  allocation  was  considered  with 
respect to diverse optimality criteria and for different types of lines.

\subsection{The Buffer Allocation Problem Formulation} \label{bap_formulation}

In the present paper, we consider buffer allocation problem for line with a series-parallel network. Example of line with a series-parallel network shown in Fig. ~\ref{lineexample}, where $M_1,…,M_7$ are machines and $B_0,…,B_5$ are buffers.

 \begin{figure}[h!]
	\centering
	\includegraphics[scale=0.9]{LineSchems}
  \caption{Example of a line with a series-parallel network\label{lineexample}}
  \end{figure}

We assume that a machine can be in an operational state or under 
repair. An operational machine may be blocked and temporarily stopped in case if there 
is  no  room  in  the  downstream  buffer.  It  may  also  be  starved  if  there  are  no  parts  to 
process  in  the  upstream  buffer. Otherwise operational machines are working. In what 
follows, m denotes the number of machines in a production line. A working machine $i$, 
$i=1,…,m$, is assumed to have a constant cycle time $C_i$ and, then, the average production 
rate $u_i=1/C_i$. 

It is supposed that machines break down only when they are working. The times to fail 
and  times  to  repair  for  each  machine  are  assumed  to  be  mutually  independent  and 
exponentially distributed random values. Let $T_b^i$ denote the average time to fail, and let 
$\lambda_i=1/T_b^i$ be the failure rate for working machine $i$, $i=1,…,m$. Similarly, let $T_r^i$ and $\mu_i=1/T_r^i$  
denote  respectively  the  time  to  repair  and  the repair rate for machine $i$. Under the 
above  mentioned  assumptions  the  system  has  the  steady  state  mode  (see  e.g. 
(Sevast'yanov,  1962)),  and  performance  of  the  system  in  this  state  is  important  for 
applications.

Let the buffers in the system be denoted by $B_1,…,B_n$ and let $h_j$ be the capacity of buffer 
$B_j$,  which  is  to  be  decided.  Denote  the  vector  of  decision  variables  as  $H=  (h_1,  h_2,…, h_n )\in  Z_+^n$,  where  $Z_+$  is  the  set  of  non-negative  integers.  

Optimization criteria that was used is:
\begin{equation}
\label{criteria}
\max \phi(H)=T_{am} R(V(H)) - Q(H) - J(H),
\end{equation}
where 
\begin{itemize}
\item $T_{am}$  amortization time of the line (line life); 
\item $V(H)$  average production rate (steady state throughput); 
\item $R(V)$  revenue related to the production rate $V$; 
\item $J(H)$ cost of buffer configuration H; 
\item $d_j$ maximal admissible capacity of buffer $B_j$, $j=1,…,n$;
\item $Q(H)= c_1q_1(H)+ …+c_n q_n(H)$ average steady state inventory cost, where $q_j(H)$ is the average steady state number of parts in buffer $b_j$, for $j=1,…,n$.
\end{itemize}

Function $\phi(H)$ has to be maximized, subject to the constraints $h_1 \leq d_1$, $h_2 \leq d_2$,…, $h_n \leq d_n$.
$R(V)$ and $J(H)$ are assumed to be given monotone non-decreasing real-valued functions. 
The cost function $J(H)$ may be non-linear to model some standard buffer capacities or 
penalize  solutions  where  the  total  capacity  of  the  buffers  exceeds  an  upper  bound. A 
non-linear revenue function $R(V)$ can model the law of diminishing returns, for example, 
it  can  reflect  the  effect  of  overproduction  by  switching  from  strictly  increasing  to 
constant at a certain threshold. A stepwise revenue function can be used to model zero 
revenue  in  case  of  an  unacceptably  low  average  production  rate  (see  e.g.  Section  3). 

In general, the production rate with finite buffers is difficult to analyze precisely with 
the Markov models. Exact performance computation of a production rate of a line with 
more than two machines and one buffer is problematic due to exponential growth of the 
number of states. Therefore, most of the techniques employed for the analysis of such 
systems are in the form of analytic approximations and simulations. Analytical 
approximations are generally based on the two-machines-one-buffer Markov models, 
and either aggregation (De Koster, 1987) or decomposition approach (Dallery et al., 
1989; Gershwin, 1987; Li, 2005). Simulation models are more expensive 
computationally but applicable to a wider class of systems (~\cite{DS95}; 
Srensen and Janssens, 2004). 
 
In this paper, we use two-machines-one-buffer Markov model independently developed 
by~\cite{LP},~\cite{DF} and~\cite{Proth84}. For 
each tentative buffer allocation decision, the production rate is evaluated via an 
aggregation algorithm (~\cite{Dolgui93};~\cite{DS95}), which is similar to the~\cite{TD87} techniques. 
This aggregation approach appears to be 
sufficiently rapid for evaluation of tentative buffer allocations within the optimization 
algorithms.  
 
The aggregation algorithm for production rate evaluation consists in recurrent 
replacement of two adjacent machines by a single machine. The parameters $\lambda^*$, $\mu^*$, $c^*$ of 
the resulting single machine are calculated from differential equations corresponding to
the two-machines-one-buffer Markov model. After $n$ steps of such aggregation 
procedure the system reduces to one virtual machine with parameters $\lambda^*$, $\mu^*$, $c^*$ and the 
estimate of the overall production rate $V(H)$ is given by $c^*\mu^/(\lambda^*+\mu^)$

\subsection{Contribution of the Paper}
We study structure of local optima for buffer allocation problem for lines with a series-parallel structure. 
Structure of local optima for each task we obtain by multi-start local descent alghoritm. After that we verify the thesis existence of a fitness landscape. On basis of obtained results, the behavior of genetic algorithm and tabu search algorithm on each task is substantiated.

\section{Investigation of Local Optimums Properties} \label{investigation}

\subsection{Fitness Landscapes}\label{subsec:landscapes}
Experimental investigation of structure of local optima in solution space of optimization problem
indicate that in many cases there is a high
concentration of local optima close to global optimum. 
This observation is known as
the thesis about the existence of a fitness landscape~\cite{Boese,Hains}. Distance to global optimum
in our case, we will calculate in the metric $l_1$.

\noindent The formulation of this thesis can be divided into two parts:
\begin{enumerate}
\item Local optima are located relatively close both to each other and to the global optimum.\newline
\item Values of the objective function of local optima tend to deteriorate with increasing distance to the global optimum.
\end{enumerate}

This thesis partly explains the performance of genetic
algorithms. If local optima are collected in the population and
the next solution is chosen somewhere between two arbitrary
local optima, then such a process has many chances to find
global optimum. In this regard, verification and theoretical
substantiation of this hypothesis is of undoubted interest.


\subsection{Tasks Used In Computational Experiment}\label{subsec:series}
For computational experiments we use three series of problems:
\begin{table}[!ht]
\centering
\small
\begin{tabular}{|c|c|c|}
\hline
Series&Number of Lines&Number of Machines\\
\hline
AS & 8 & 4 -- 14 \\
BN & 10 & 5 \\
VP & 4 & 5 \\
\hline
\end{tabular}
\caption{List of series}\label{tabl:series}
\vspace{-0.5cm}
\end{table}

The AS series consists of tasks created from lines~1,2,6,7,8 from~\cite{Ancelin1987} with real data from
produced by Renault.
A distinctive feature of the 7 and 8 lines is the presence
parallel sections (see figures~\ref{fig:vis_as7},~\ref{fig:vis_as8}).
Line parameters~1,2,6,7,8 are given in tables \ref{tabl:as1}-\ref{tabl:as8}.

 \begin{figure}[h!]
	\centering
	\includegraphics[scale=0.6]{ans7}
  \caption{Line structure AS7} \label{fig:vis_as7}
  \end{figure}

 \begin{figure}[h!]
	\centering
	\includegraphics[scale=0.55]{ans8}
  \caption{Line structure AS8} \label{fig:vis_as8}
  \end{figure}


\begin{table}[h!]
\centering
\small
\begin{tabular}{||c|c||c|c|c|c||}
\hline \multicolumn{2}{||c||}{buffers}&\multicolumn{4}{|c||}{machines}\\
\hline
$i$ & $d_i$ & $j$ & $T^{\rm O}_j$ & $T^{\rm B}_j$ & $U_i$\\
\hline
1 & 20 & 1 & 244.2 & 150 & 10\\
2 & 17 & 2 & 255.3 & 300 & 10\\
3 & 38 & 3 & 176 & 75 & 10\\
4 & 48 & 4 & 184 & 600 & 10\\
&& 5 & 192 & 450 & 10\\
\hline
\end{tabular}\\
\caption{Task parameters $\textit{AS1}$}\label{tabl:as1}
\end{table}

\begin{table}[h!]
	\centering
	\small
	\begin{tabular}{||c|c||c|c|c|c||}
		\hline \hline
		\multicolumn{2}{||c||}{Buffers}&\multicolumn{4}{|c||}{Machines}\\
		\hline
		$i$ & $d_i$ & $j$  & $T^{\rm O}_j$ & $T^{\rm B}_j$ & $U_i$ \\
		\hline
		1 & 0   & 1  & 10000 & 440 & 22 \\
		2 & 50  & 2  & 20000 & 440 & 23 \\
		3 & 20  & 3  & 5000  & 430 & 22 \\
		4 & 50  & 4  & 40000 & 520 & 23 \\
		5 &  0  & 5  & 30000 & 430 & 24 \\
		6 & 80  & 6  & 2442  & 440 & 22 \\
		7 & 20  & 7  & 1840  & 520 & 23 \\
		8 & 100 & 8  & 1680  & 430 & 21 \\
		9 & 100 & 9  & 2208  & 920 & 24 \\
		\hline
	\end{tabular}
	\caption{Task parameters $\textit{AS2}$} \label{tabl:as2}
\end{table}  
\begin{table}[h!]
	\centering
	\small
	\begin{tabular}{||c|c||c|c|c|c||}
		\hline \hline
		\multicolumn{2}{||c||}{Buffers}&\multicolumn{4}{|c||}{Machines}\\
		\hline
		$i$ & $d_i$ & $j$  & $T^{\rm O}_j$ & $T^{\rm B}_j$ & $U_i$ \\
		\hline
		1  & 60 & 1 & 29880 & 22000 &  385\\
		2  & 60 & 2 & 29880 & 22000 & 426\\
		3  & 50 & 3 & 876000 & 22300 & 330 \\
		4  & 70 & 4 & 29880 & 22000 & 372\\
		5  & 60 & 5 & 33250 & 27500 & 316 \\
		6  & 80 & 6 & 144000 & 8500 & 340 \\
		7  & 45 & 7 & 102300 & 74000 & 340 \\
		8  & 25 & 8 & 113300 & 7200 & 340 \\
		9  & 35 & 9 & 540000 &  60000 & 380 \\
		10 & 80 & 10 & 538800 & 349000 & 350 \\
		11 & 40 & 11 & 5064000 & 73700 & 400 \\
		12 & 45 & 12 & 468000 & 306000 & 400 \\
		13 & 65 & 13 & 1032000 & 54000 & 319 \\
		& & 14 & 45600 & 31120 & 319 \\
		\hline
	\end{tabular}
	\caption{Task parameters $\textit{AS6}$} \label{tabl:as6}
\end{table}  

\begin{table}[h!]
	\centering
	\small
	\begin{tabular}{||c|c||c|c|c|c||}
		\hline \hline
		\multicolumn{2}{||c||}{Buffers}&\multicolumn{4}{|c||}{Machines}\\
		\hline
		$i$ & $d_i$ & $j$  & $T^{\rm O}_j$ & $T^{\rm B}_j$ & $U_i$ \\
		\hline
		1 & 15 & 1 & 50000 & 12000 & 1000\\
		2 & 10 & 2 & 48000 & 2000 & 3450\\
		3 & 15 & 3 & 55000 & 9000 & 2780 \\
		4 & 10 & 4 & 39000 & 6000 & 3030\\
		5 &  25  & 5 & 75000 & 10000 & 3333\\
		6 &  10 & 6 & 59000 & 11000 & 2560\\
		7 & 10 & 7 & 28000 & 8000 & 3030\\
			&  & 8 & 35000 & 8000 & 3125\\
			&  & 9 & 65000 &  35000 & 2174\\
			&  & 10 & 20000 & 4000 & 800\\
		\hline \hline
	\end{tabular}
	\caption{Task parameters $\textit{AS7}$} \label{tabl:as7}
\end{table}
\begin{table}[h!]
	\centering
	\small
	\begin{tabular}{||c|c||c|c|c|c||}
		\hline \hline
		\multicolumn{2}{||c||}{Buffers}&\multicolumn{4}{|c||}{Machines}\\
		\hline
		$i$ & $d_i$ & $j$  & $T^{\rm O}_j$ & $T^{\rm B}_j$ & $U_i$ \\
		\hline
		1 & 1300 & 1 & 87000 & 27000 & 23\\
		2 & 200 & 2 & 77000 & 22000 & 27\\
		3 & 0 & 3 & 580000 & 18000 & 38\\
		4 & 0 & 4 & 410000 & 12500 & 30\\
		5 & 0 & 5 & 580000 & 18000 & 38\\
		6 & 2000 & 6 & 410000 & 12500 & 30\\
		7 & 0 & 7 & 725000 & 21000 & 20\\
		  &  & 8 & 550000 & 14000 & 40\\
		 &  & 9 & 430000 & 24000 & 43\\
		   &  & 10 & 270000 & 22000 & 33\\
		\hline \hline
	\end{tabular}
	\caption{Task parameters $\textit{AS8}$} \label{tabl:as8}
\end{table}

bn5.1-bn5.10 series consists of 10 lines with 5 machines in
each from~\cite{eng1}. There are two bottlenecks in the problems of this series. Under
bottleneck is understood as a line section of two relatively slow machines and a buffer
between them. 

Series vp6.9-vp6.10 and vp7.9-vp7.10~\cite{vp} are composed of problems on lines from
5 consecutive machines from~\cite{vp}. 


\subsection{Computational Experiment} \label{subsec:experiment}

To verify the thesis, a method was developed
for finding the number of integer points in
a ball of a given radius in the metric $l_1$ at the intersection with a parallelepiped
whose faces are parallel to coordinate planes.
The method
was obtained by reducing the problem to a combinatorial
formulation, already considered earlier for the case of non-negative
integer points, using
generating functions ~\cite{Sach}.

A series of experiments were carried out on existing tasks for
determination of the structure of local optima. With a one-time
running the local lifting algorithm LSA solution $H$ whose elements $h_i$ are chosen with
uniform distribution between 1 and $d_i$ improved to a local optimum.
This procedure was repeated 300 times to create the necessary
sample size~\cite{Bose}. Based on the results obtained, it was calculated
the number of admissible solutions $\mid \Omega \mid$ in balls containing all found
local optima. In this case, the distance from
best found local optimum to the worst, and beyond the center -
the best local optimum found.

In table~\ref{spher_as}-\ref{spher_bn} the column $V1$ contains the cardinality $\mid \Omega \mid$ for balls containing
local
optima, the column $V2$ contains the cardinality of the entire space
solutions for each problem, and the column $V1/V2$ is their ratio.
As can be seen from the tables, the first part of the thesis about the existence of a <<central mountain range>>, that
local optima are located relatively close both to each other and to the global optimum,
in our case, it was not performed and mainly in the tasks used
the optima are strongly scattered throughout the solution space.

\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		\hspace*{0.1cm}№\hspace*{0.1cm} &
		\hspace*{0.1cm}$V1$\hspace*{0.1cm}&
		\hspace*{0.1cm}$V2$\hspace*{0.1cm}&
		\hspace*{0.1cm}$V1/V2$\hspace*{0.1cm}\\
		\hline
		1 &  2,84E+05  &  5,74E+05 &   0,49\\
		2 &  8,88E+11  &  9,48E+11 &   0,94\\
		3 &  8,60E+01  &  4,85E+03 &   0,02\\
		4 &  2,34E+22  &  2,89E+22 &   0,81\\
		5 &  4,17E+07  &  9,75E+07 &   0,43\\
		6 &  1,00E+00  &  5,23E+08 &   0,00\\
		\hline
	\end{tabular}
	%\vspace{1em}
	\caption{Results of running a local search multiple times for a series as.1 - as.6}	\label{spher_as}
\end{table}
\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		\hspace*{0.1cm}№\hspace*{0.1cm} &
		\hspace*{0.1cm}$V1$\hspace*{0.1cm}&
		\hspace*{0.1cm}$V2$\hspace*{0.1cm}&
		\hspace*{0.1cm}$V1/V2$\hspace*{0.1cm}\\
		\hline
		9  & 7,97E+03  &  1,00E+04  &  0,80\\
		10 & 7,55E+03  &  1,46E+04  &  0,52\\
		\hline
	\end{tabular}
	%\vspace{1em}
	\caption{Results of running a local search multiple times for a series vp6.9 - vp6.10}	\label{spher_vp6}
\end{table}
\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		\hspace*{0.1cm}№\hspace*{0.1cm} &
		\hspace*{0.1cm}$V1$\hspace*{0.1cm}&
		\hspace*{0.1cm}$V2$\hspace*{0.1cm}&
		\hspace*{0.1cm}$V1/V2$\hspace*{0.1cm}\\
		\hline
		9  & 6,50E+03  &  1,00E+04  &  0,65\\
		10 & 9,95E+03  &  1,46E+04  &  0,68\\
		\hline
	\end{tabular}
	%\vspace{1em}
	\caption{Results of running a local search multiple times for a series vp7.9 - vp7.10}	\label{spher_vp7}
\end{table}

Also considered was a part of the thesis about the tendency to worsen the value of the objective function of local optima
with increasing distance to the global optimum. To do this, for all the considered problems, the correlation was determined
$\rho(\varphi(\xi),r(\xi,\xi^*))$ objective function values
local optima $\varphi(\xi)$ and the distance to the global optimum $r(\xi,\xi^*)$.
As shown by experiments on the proposed problems, there is a negative correlation $\rho$, and all values
correlations are significantly different from 0 with a confidence level of 95\% (tables~\ref{res_korr_as_vp}--\ref{res_korr_bn_vp}).
\begin{table}[h!]
	\vspace{1cm}
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		\hspace*{0.1cm}№\hspace*{0.1cm} &
		\hspace*{0.1cm}$V1$\hspace*{0.1cm}&
		\hspace*{0.1cm}$V2$\hspace*{0.1cm}&
		\hspace*{0.1cm}$V1/V2$\hspace*{0.1cm}\\
		\hline
		1 &  1,44E+05  &  1,94E+05  &  0,74\\
		2 &  9,96E+03  &  1,94E+05  &  0,05\\
		3 &  9,07E+04  &  1,94E+05  &  0,47\\
		4 &  2,06E+04  &  1,94E+05  &  0,11\\
		5 &  6,95E+04  &  1,94E+05  &  0,36\\
		6 &  6,94E+04  &  1,94E+05  &  0,36\\
		7 &  2,45E+04  &  1,94E+05  &  0,13\\
		8 &  3,42E+03  &  1,94E+05  &  0,02\\
		9 &  6,28E+04  &  1,94E+05  &  0,32\\
		10&  1,00E+00  &  1,94E+05  &  0,00\\
		\hline
	\end{tabular}
	\vspace{1em}
	\caption{Results of running a local search multiple times for a series bn5.1 - bn5.10}	\label{spher_bn}
\end{table}
\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|}
		\hline
		%\hspace*{2mm} № \hspace*{2mm} & \hspace*{5.1cm} $\rho(r(\xi, \xi*),\varphi(\xi))$\hspace*{5.1cm}\\
		\hspace*{2mm} № \hspace*{2mm} & \hspace*{1cm} $\rho(r(\xi, \xi*),\varphi(\xi))$\hspace*{1cm}\\
		\hline
		\multicolumn{2}{|c|}{Series as.1 - as.5}\\
		\hline
		1  & -0,87078206\\
		2  & -0,437208613\\
		7  & -0,547348483\\
		8  & -0,943725905\\
		\hline
		\multicolumn{2}{|c|}{Series vp6.9 - bn6.10}\\
		\hline
		9  & -0,833583818\\
		10 & -0,778237651\\
		\hline	
	\end{tabular}
	\vspace{1em}
	\caption{Results of running a local search multiple times for a series $as$ and $vp6$}	\label{res_korr_as_vp}
\end{table}

\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|}
		\hline
		\hspace*{2mm} № \hspace*{2mm} & \hspace*{1cm} $\rho(r(\xi, \xi*),\varphi(\xi))$\hspace*{1cm}\\
		\hline
		\multicolumn{2}{|c|}{Series bn5.1 - bn5.9}\\
		\hline
		1  & -0,706451573\\
		2  & -0,872602935\\
		3  & -0,914939714\\
		4  & -0,999969666\\
		5  & -0,972555479\\
		6  & -0,729655068\\
		7  & -0,973827419\\
		8  & -0,846807033\\
		9  & -0,884916669\\
		\hline
		\multicolumn{2}{|c|}{Series vp7.9 - bn7.10}\\
		\hline
		9  & -0,907792301\\
		10 & -0,889724863\\		
		\hline		
	\end{tabular}
	\vspace{1em}
	\caption{Correlation results for series $bn$ and $vp7$}	\label{res_korr_bn_vp}
\end{table}

An interesting fact is that
that in problems as.6, bn5.1 the entire set of local optima splits
into clusters, for each of which a part of the thesis about the tendency to
deterioration of the value of the objective function of local optima
with increasing distance to the global optimum.

Figures~\ref{klaster_as4} and ~\ref{klaster_bn5_1} show diagrams of local optima for problems
as.6, bn5.1, where the ordinate shows the value of the objective function of the local optimum $\varphi(H)$, and the abscissa is
distance in metric $l_1$ to the chosen solution.

 \begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{multistart_klaster}
  \caption{Local optimums obtained by local descent multistart } \label{fig:multistart_klaster}
  \end{figure}
 \begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{ga_klaster}
  \caption{Final population of GA} \label{fig:ga_klaster}
  \end{figure}
 \begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{gals_klaster}
  \caption{Final population of GA with local descent} \label{fig:gals_klaster}
  \end{figure}

The effect of clustering is especially well manifested in problems with parallel sections of the line.
This effect can be justified by the fact that there are several different paths in lines with a parallel structure.
from start bin to end bin. Thus, if you create two lines that are identical in graph structure,
such that one line will have relatively large buffers along the same path, and the other -
otherwise, it is possible to achieve two solutions close in value to the objective function, but located on <<enough>>
great distance from each other.



\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.2]{klaster_as4.png}\\
		\caption{The structure of local optima of the problem as6.} \label{klaster_as4}
	\end{center}
\end{figure} 
\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.2]{klaster_bn5_1.png}\\
		\caption{The structure of local optima of the problem bn5.1.} \label{klaster_bn5_1}
	\end{center}
\end{figure} 

Another reason for the clustering effect is the property
line symmetry. This property is set to~\cite{LP} for a serial line,
consisting of two machine and a buffer
between them. If we swap the parameters of the first and second EOs so that
that the input section of the line will become the output, and the output - the input, then
line performance will not change.

Consider the example of serial line t.1 from
%{\frac{1}{T^{\rm O}_j} = 2U_j}, {\frac{1}{T^{\rm B}_j}= 4U_j}, {U_j = U_{j+1} }
three HUs and two hoppers between them with the following parameters: ${T^{\rm O}_i=T^{\rm B}_i=1}$, $i = 1,\ldots,3$;
 ${U_1=1}$, ${U_2=0.5}$ , ${U_3=1}$;
 ${d_j=4}$, ${c_j=0}$, $j = 1,\ldots,2$;
 $T_{am}=7000$;
 $J(H)=50\cdot(h_1+h_2)$;
 if $V(H) < 2570$ then $R(V(H))=0.9\cdot V(H)$, otherwise $R(V(H))=2570$.

 \begin{figure}[h!]
	\centering
	\includegraphics[scale=0.9]{test}
  \caption{Values of the objective function on the set $D$ for problem t.1\label{t_1}}
  \end{figure}

The set of global optima (see Fig.~\ref{t_1}) of this example splits into two clusters. IN
the first cluster contains solutions $H^1=(1,2)$ and $H^2=(1,3)$, and
the second is the solutions $H^3=(2,1)$ and $H^4=(3,1)$. Clustering optima in
considered example is a consequence of the symmetry and singularities
aggregation algorithm from~\cite{Dol} used in calculating the average
line performance.

In the general case, for lines with $n\geq 3$ bins, the effect
symmetry can be observed in several internal areas
lines. This leads to the fact that with this aggregation algorithm
the set of local optima is divided into several clusters.

\section{Conclusion}

\begin{enumerate}

\item Cluster structure of local optima is established of some tasks having parallel parts in structure.

\item Reasons for the emergence of clusters of local optima are determined..

\end{enumerate}

\bibliography{eremeev_et_al}   
                                                  
\end{document}


